{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> Project 2\n",
    "\n",
    "Project Description:\n",
    "- Use same datasets as Project 1.\n",
    "- Preprocess data: Explore data and apply data scaling.\n",
    "\n",
    "Regression Task:\n",
    "- Apply any two models with bagging and any two models with pasting.\n",
    "- Apply any two models with adaboost boosting\n",
    "- Apply one model with gradient boosting\n",
    "- Apply PCA on data and then apply all the models in project 1 again on data you get from PCA. Compare your results with results in project 2. You don't need to apply all the models twice. Just copy the result table from project 1, prepare similar table for all the models after PCA and compare both tables. Does PCA help in getting better results?\n",
    "- Apply deep learning models covered in class\n",
    "\n",
    "Classification Task:\n",
    "- Apply two voting classifiers - one with hard voting and one with soft voting\n",
    "- Apply any two models with bagging and any two models with pasting.\n",
    "- Apply any two models with adaboost boosting\n",
    "- Apply one model with gradient boosting\n",
    "- Apply PCA on data and then apply all the models in project 1 again on data you get from PCA. Compare your results with results in project 1. You don't need to apply all the models twice. Just copy the result table from project 1, prepare similar table for all the models after PCA and compare both tables. Does PCA help in getting better results?\n",
    "- Apply deep learning models covered in class\n",
    "\n",
    "Deliverables:\n",
    "- Use markdown to provide inline comments for this project.\n",
    "- Your outputs should be clearly executed in the notebook i.e. we should not need to rerun the code to obtain the outputs.\n",
    "- Visualization encouraged.\n",
    "- If you are submitting two different files, then please only one group member submit both the files. If you submit two files separately from different accounts, it will be submitted as two different attempts.\n",
    "- If you are submitting two different files, then please follow below naming convetion:\n",
    "    Project2_Regression_GroupXX_Firstname1_Firstname2.ipynb\n",
    "    Project2_Classification_GroupXX_Firstname1_Firstname2.ipynb\n",
    "- If you are submitting single file, then please follow below naming convetion:\n",
    "    Project2_Both_GroupXX_Firstname1_Firstname2.ipynb\n",
    "\n",
    "Questions regarding the project:\n",
    "- We have created a discussion board under Projects folder on e-learning. Create threads over there and post your queries related to project there.\n",
    "- We will also answer queries there. We will not be answering any project related queries through the mail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply any two models with bagging and any two models with pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(action = \"ignore\")\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('master_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sector_score_x</th>\n",
       "      <th>LOCATION_ID_x</th>\n",
       "      <th>PARA_A_x</th>\n",
       "      <th>Score_A</th>\n",
       "      <th>Risk_A</th>\n",
       "      <th>PARA_B_x</th>\n",
       "      <th>Score_B</th>\n",
       "      <th>Risk_B</th>\n",
       "      <th>TOTAL_x</th>\n",
       "      <th>numbers_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Marks</th>\n",
       "      <th>Money_Value_y</th>\n",
       "      <th>MONEY_Marks</th>\n",
       "      <th>District</th>\n",
       "      <th>Loss</th>\n",
       "      <th>LOSS_SCORE</th>\n",
       "      <th>History_y</th>\n",
       "      <th>History_score</th>\n",
       "      <th>Score_y</th>\n",
       "      <th>Risk_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.89</td>\n",
       "      <td>23</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.508</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500</td>\n",
       "      <td>6.68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.966</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.74</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.80</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.480</td>\n",
       "      <td>10.80</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>11.75</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sector_score_x  LOCATION_ID_x  PARA_A_x  Score_A  Risk_A  PARA_B_x  \\\n",
       "0            3.89             23      4.18      0.6   2.508      2.50   \n",
       "1            3.89              6      0.00      0.2   0.000      4.83   \n",
       "2            3.89              6      0.51      0.2   0.102      0.23   \n",
       "3            3.89              6      0.00      0.2   0.000     10.80   \n",
       "4            3.89              6      0.00      0.2   0.000      0.08   \n",
       "\n",
       "   Score_B  Risk_B  TOTAL_x  numbers_x  ...  Marks  Money_Value_y  \\\n",
       "0      0.2   0.500     6.68        5.0  ...      2           3.38   \n",
       "1      0.2   0.966     4.83        5.0  ...      2           0.94   \n",
       "2      0.2   0.046     0.74        5.0  ...      2           0.00   \n",
       "3      0.6   6.480    10.80        6.0  ...      6          11.75   \n",
       "4      0.2   0.016     0.08        5.0  ...      2           0.00   \n",
       "\n",
       "   MONEY_Marks  District  Loss  LOSS_SCORE  History_y  History_score  Score_y  \\\n",
       "0            2         2     0           2          0              2      2.4   \n",
       "1            2         2     0           2          0              2      2.0   \n",
       "2            2         2     0           2          0              2      2.0   \n",
       "3            6         2     0           2          0              2      4.4   \n",
       "4            2         2     0           2          0              2      2.0   \n",
       "\n",
       "   Risk_y  \n",
       "0       1  \n",
       "1       0  \n",
       "2       0  \n",
       "3       1  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting target variables y = Audit_Risk for regression  y2= Risk_x for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(776, 45)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df['Audit_Risk']\n",
    "y2=df['Risk_x']\n",
    "X=df.drop(['Audit_Risk','Risk_x'],axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sector_score_x</th>\n",
       "      <th>LOCATION_ID_x</th>\n",
       "      <th>PARA_A_x</th>\n",
       "      <th>Score_A</th>\n",
       "      <th>Risk_A</th>\n",
       "      <th>PARA_B_x</th>\n",
       "      <th>Score_B</th>\n",
       "      <th>Risk_B</th>\n",
       "      <th>TOTAL_x</th>\n",
       "      <th>numbers_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Marks</th>\n",
       "      <th>Money_Value_y</th>\n",
       "      <th>MONEY_Marks</th>\n",
       "      <th>District</th>\n",
       "      <th>Loss</th>\n",
       "      <th>LOSS_SCORE</th>\n",
       "      <th>History_y</th>\n",
       "      <th>History_score</th>\n",
       "      <th>Score_y</th>\n",
       "      <th>Risk_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.89</td>\n",
       "      <td>23</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.508</td>\n",
       "      <td>2.50</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.500</td>\n",
       "      <td>6.68</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3.38</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.83</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.966</td>\n",
       "      <td>4.83</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.94</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.74</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.80</td>\n",
       "      <td>0.6</td>\n",
       "      <td>6.480</td>\n",
       "      <td>10.80</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>11.75</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.89</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sector_score_x  LOCATION_ID_x  PARA_A_x  Score_A  Risk_A  PARA_B_x  \\\n",
       "0            3.89             23      4.18      0.6   2.508      2.50   \n",
       "1            3.89              6      0.00      0.2   0.000      4.83   \n",
       "2            3.89              6      0.51      0.2   0.102      0.23   \n",
       "3            3.89              6      0.00      0.2   0.000     10.80   \n",
       "4            3.89              6      0.00      0.2   0.000      0.08   \n",
       "\n",
       "   Score_B  Risk_B  TOTAL_x  numbers_x  ...  Marks  Money_Value_y  \\\n",
       "0      0.2   0.500     6.68        5.0  ...      2           3.38   \n",
       "1      0.2   0.966     4.83        5.0  ...      2           0.94   \n",
       "2      0.2   0.046     0.74        5.0  ...      2           0.00   \n",
       "3      0.6   6.480    10.80        6.0  ...      6          11.75   \n",
       "4      0.2   0.016     0.08        5.0  ...      2           0.00   \n",
       "\n",
       "   MONEY_Marks  District  Loss  LOSS_SCORE  History_y  History_score  Score_y  \\\n",
       "0            2         2     0           2          0              2      2.4   \n",
       "1            2         2     0           2          0              2      2.0   \n",
       "2            2         2     0           2          0              2      2.0   \n",
       "3            6         2     0           2          0              2      4.4   \n",
       "4            2         2     0           2          0              2      2.0   \n",
       "\n",
       "   Risk_y  \n",
       "0       1  \n",
       "1       0  \n",
       "2       0  \n",
       "3       1  \n",
       "4       0  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-8d5eb4e163f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"white\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcorr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"white\")\n",
    "corr = X.corr()\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature Selection by using correlation, using threshold of 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(dataset, threshold):\n",
    "    col_corr = set() # Set of all the names of deleted columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):\n",
    "                colname = corr_matrix.columns[i] # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "                if colname in dataset.columns:\n",
    "                    del dataset[colname] # deleting the column from the dataset\n",
    "\n",
    "    print(list(dataset))\n",
    "\n",
    "correlation(X,0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance - to get the most relevent features to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X,y2)\n",
    "print(model.feature_importances_)\n",
    "feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "feat_importances.plot(kind='barh')\n",
    "plt.xlabel('Feature Importance', fontsize=20)\n",
    "plt.ylabel('Features', fontsize=20)\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "plt.tick_params(axis='both', which='minor', labelsize=10)\n",
    "plt.figure(figsize=(70,10))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted = X.columns[X.columns.str.startswith('LOC')]\n",
    "X.drop(unwanted, axis=1, inplace=True)\n",
    "X.shape,list(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting X and y into train and test version\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y2, random_state=0)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##- Apply two voting classifiers - one with hard voting and one with soft voting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bagging classification  with decision tree classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "bag_clf = BaggingClassifier(dt_clf, n_estimators=500, max_samples=100, bootstrap=True, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "print('Train score: {:.2f}'.format(bag_clf.score(X_train, y_train)))\n",
    "print('Test score: {:.2f}'.format(bag_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=0)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_range = [1,2,3,4,5,6,7,8,9,10]\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for d in depth_range:\n",
    "    #dtree = DecisionTreeClassifier(max_depth=d,random_state=0)\n",
    "    bag_clf.fit(X_train, y_train)\n",
    "    #dtree.fit(X_train_org, y_train)\n",
    "    train_score.append(bag_clf.score(X_train, y_train))\n",
    "    test_score.append(bag_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(depth_range, train_score, label = 'Train score')\n",
    "plt.plot(depth_range, test_score, label = 'Test score')\n",
    "plt.legend()\n",
    "plt.xlabel('Maximum depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot not getting... (PENDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib.colors  import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.title(\"Decision Tree\", fontsize=14)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(bag_clf, X, y)\n",
    "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bagging with Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=0)\n",
    "bag_clf = BaggingClassifier(dt_clf, n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_pred == y_pred_rf ) / len(y_pred)  # almost identical predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "#rnd_clf = DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=0)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100 ,n_jobs=-1, random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "bag_clf1 = BaggingClassifier(rnd_clf, n_estimators=100, max_samples=0.5, bootstrap=True, random_state=0)\n",
    "bag_clf1.fit(X_train, y_train)\n",
    "y_pred1 = bag_clf1.predict(X_test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree1 = tree_clf1.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_tree1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-Bag evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "bag_clf = BaggingClassifier(dt_clf, n_estimators=500, bootstrap=True, n_jobs=-1, oob_score=True, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_clf.oob_decision_function_[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "y_list = rnd_clf.feature_importances_\n",
    "y_pos = np.arange(len(y_list))\n",
    "features = X.columns.get_values()\n",
    "plt.barh(y_pos, y_list, align='center', alpha=0.4)\n",
    "plt.yticks(y_pos,features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For pasting set bootstrap=False "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pasting classification with decision tree classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "bag_clf = BaggingClassifier(dt_clf, n_estimators=500, max_samples=100, bootstrap=False, random_state=0)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "print('Train score: {:.2f}'.format(bag_clf.score(X_train, y_train)))\n",
    "print('Test score: {:.2f}'.format(bag_clf.score(X_test, y_test)))\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(random_state=0)\n",
    "tree_clf.fit(X_train, y_train)\n",
    "y_pred_tree = tree_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_tree))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dtree\n",
    "depth_range = [1,2,3,4,5,6,7,8,9,10]\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for d in depth_range:\n",
    "    #dtree = DecisionTreeClassifier(max_depth=d,random_state=0)\n",
    "    bag_clf.fit(X_train, y_train)\n",
    "    #dtree.fit(X_train_org, y_train)\n",
    "    train_score.append(bag_clf.score(X_train, y_train))\n",
    "    test_score.append(bag_clf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(depth_range, train_score, label = 'Train score')\n",
    "plt.plot(depth_range, test_score, label = 'Test score')\n",
    "plt.legend()\n",
    "plt.xlabel('Maximum depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot not getting... (PENDING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  matplotlib.colors  import ListedColormap\n",
    "\n",
    "def plot_decision_boundary(clf, X, y, axes=[-1.5, 2.5, -1, 1.5], alpha=0.5, contour=True):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "    if contour:\n",
    "        custom_cmap2 = ListedColormap(['#7d7d58','#4c4c7f','#507d50'])\n",
    "        plt.contour(x1, x2, y_pred, cmap=custom_cmap2, alpha=0.8)\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", alpha=alpha)\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", alpha=alpha)\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
    "    plt.ylabel(r\"$x_2$\", fontsize=18, rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plt.figure(figsize=(11,4))\n",
    "plt.subplot(121)\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "plt.title(\"Decision Tree\", fontsize=14)\n",
    "plt.subplot(122)\n",
    "plot_decision_boundary(bag_clf, X, y)\n",
    "plt.title(\"Decision Trees with Bagging\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pasting with Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf = DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=0)\n",
    "bag_clf = BaggingClassifier(dt_clf, n_estimators=500, max_samples=1.0, bootstrap=False, n_jobs=-1, random_state=0)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(y_pred == y_pred_rf ) / len(y_pred)  # almost identical predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "#rnd_clf = DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16, random_state=0)\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100 ,n_jobs=-1, random_state=0)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "bag_clf1 = BaggingClassifier(rnd_clf, n_estimators=100, max_samples=0.5, bootstrap=False, random_state=0)\n",
    "bag_clf1.fit(X_train, y_train)\n",
    "y_pred1 = bag_clf1.predict(X_test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_clf.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree1 = tree_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred_tree1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-Bag evaluation estimation only available if bootstrap=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "y_list = rnd_clf.feature_importances_\n",
    "y_pos = np.arange(len(y_list))\n",
    "features = X.columns.get_values()\n",
    "plt.barh(y_pos, y_list, align='center', alpha=0.4)\n",
    "plt.yticks(y_pos,features)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ada boosting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ada boosting with Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, algorithm=\"SAMME.R\", learning_rate=0.5, random_state=0)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "boosting_pred = ada_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "acc_score = accuracy_score(y_test, boosting_pred)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\",precision_score(y_test, boosting_pred))\n",
    "\n",
    "\n",
    "print(\"Recall:\",recall_score(y_test, boosting_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix_boosting = confusion_matrix(y_test, boosting_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Function to plot confusion matrix\n",
    "def plot_conf_matrix (confusion_matrix):\n",
    "    class_names = [0,1]\n",
    "    fontsize=14\n",
    "    df_conf_matrix = pd.DataFrame(\n",
    "            confusion_matrix, index=class_names, columns=class_names, \n",
    "        )\n",
    "    fig = plt.figure()\n",
    "    heatmap = sns.heatmap(df_conf_matrix, annot=True, fmt=\"d\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_conf_matrix(conf_matrix_boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNC TOPLOT ROC_CURVE\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "def plot_roc_curve(roc_auc):\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, boosting_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plot_roc_curve(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "plot_decision_boundary(ada_clf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "m = len(X_train)\n",
    "\n",
    "plt.figure(figsize=(11, 4))\n",
    "for  subplot,  learning_rate  in ((121, 1), (122, 0.5)):\n",
    "    sample_weights = np.ones(m)\n",
    "    plt.subplot(subplot)\n",
    "    for i in range(5):\n",
    "        svm_clf = SVC(kernel=\"rbf\", C=0.05, random_state=42)\n",
    "        svm_clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "        y_pred = svm_clf.predict(X_train)\n",
    "        sample_weights[y_pred != y_train] *= (1 + learning_rate)\n",
    "        plot_decision_boundary(svm_clf, X, y, alpha=0.2)\n",
    "        plt.title(\"learning_rate = {}\".format(learning_rate), fontsize=16)\n",
    "    if subplot == 121:\n",
    "        plt.text(-0.7, -0.65, \"1\", fontsize=14)\n",
    "        plt.text(-0.6, -0.10, \"2\", fontsize=14)\n",
    "        plt.text(-0.5,  0.10, \"3\", fontsize=14)\n",
    "        plt.text(-0.4,  0.55, \"4\", fontsize=14)\n",
    "        plt.text(-0.3,  0.90, \"5\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)\n",
    "    print('Model Performance')\n",
    "    print('Mean_Absolute_Error:', metrics.mean_absolute_error(y_test, predictions))\n",
    "    print('Mean_Squared_Error:', metrics.mean_squared_error(y_test, predictions))\n",
    "    print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Create the parameter distribution\n",
    "param_dist = {'n_estimators': [int(x) for x in np.linspace(start = 100, stop = 500, num = 10)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'max_depth': [1, 20]}\n",
    "\n",
    "RFR = RandomForestClassifier()\n",
    "RFR_cv = RandomizedSearchCV(RFR, param_dist)\n",
    "RFR_cv.fit(X_train, y_train)\n",
    "evaluate(RFR_cv.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFR_cv.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada  Boosting Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = np.random.RandomState(0)\n",
    "param_dist = {'n_estimators': [int(x) for x in np.linspace(start = 100, stop = 400, num = 10)]}\n",
    "\n",
    "\n",
    "ada = AdaBoostClassifier(RFR, random_state = 0)\n",
    "ada_cv = RandomizedSearchCV(ada, param_dist)\n",
    "ada_cv.fit(X_train, y_train)\n",
    "\n",
    "evaluate(ada_cv.best_estimator_, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining the best estimators from the model\n",
    "print(ada_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making Predictions\n",
    "boosting_pred = ada_cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "acc_score = accuracy_score(y_test, boosting_pred)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision:\",precision_score(y_test, boosting_pred))\n",
    "\n",
    "\n",
    "print(\"Recall:\",recall_score(y_test, boosting_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix_boosting = confusion_matrix(y_test, boosting_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Function to plot confusion matrix\n",
    "def plot_conf_matrix (confusion_matrix):\n",
    "    class_names = [0,1]\n",
    "    fontsize=14\n",
    "    df_conf_matrix = pd.DataFrame(\n",
    "            confusion_matrix, index=class_names, columns=class_names, \n",
    "        )\n",
    "    fig = plt.figure()\n",
    "    heatmap = sns.heatmap(df_conf_matrix, annot=True, fmt=\"d\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_conf_matrix(conf_matrix_boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNC TOPLOT ROC_CURVE\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "def plot_roc_curve(roc_auc):\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [0, 1],'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, threshold = roc_curve(y_test, boosting_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plot_roc_curve(roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=1, n_estimators=300, learning_rate=1.0, random_state=0)\n",
    "gbrt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=gbrt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "rmse_test = mean_squared_error(y_test,y_pred)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test set RMSE:{:.2f}'.format(rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot not getting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.figure(figsize=(11,4))\n",
    "\n",
    "plt.subplot(121)\n",
    "plot_predictions([gbrt], X_train[:,[2]], y_train, axes=[-0.5, 0.5, -0.1, 0.8], label=\"Ensemble predictions\")\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt.learning_rate, gbrt.n_estimators), fontsize=14)\n",
    "\n",
    "plt.subplot(122)\n",
    "plot_predictions([gbrt_slow], X, y, axes=[-0.5, 0.5, -0.1, 0.8])\n",
    "plt.title(\"learning_rate={}, n_estimators={}\".format(gbrt_slow.learning_rate, gbrt_slow.n_estimators), fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# voting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hard voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "voting_clf = VotingClassifier(\n",
    "estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "voting='hard'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# soft voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC(probability=True)\n",
    "voting_clf = VotingClassifier(\n",
    "estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "voting='soft'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pca - 95% explained variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "X_train_transformed = pca.fit_transform(X_train)\n",
    "X_test_transformed  = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pca 95% variation has reduced from 12 features to 7 feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# implementing all models from part 1 with reduced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "We will run following models for classification:\n",
    "KNN classifcation, Logistic Regression, Linear Supprt Vector Machine, Kerenilzed Support Vector Machine, Decision Tree.\n",
    "Models will be evaluated on their Classification accuracy-the number of correct predictions made as a ratio of all predictions made and confusion matrix of respective models\n",
    "KNN Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "train_score_array = []\n",
    "test_score_array = []\n",
    "\n",
    "for k in range(1,20):\n",
    "    knn = KNeighborsClassifier(k)\n",
    "    knn.fit(X_train_transformed, y_train)\n",
    "    train_score_array.append(knn.score(X_train_transformed, y_train))\n",
    "    test_score_array.append(knn.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = range(1,20)\n",
    "%matplotlib inline\n",
    "plt.plot(x_axis, train_score_array, label = 'Train Score', c = 'g')\n",
    "plt.plot(x_axis, test_score_array, label = 'Test Score', c='b')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(np.arange(0, 20, step=1))\n",
    "plt.legend()\n",
    "#plt.setp(fontsize=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k = 4 gives best result which is same without PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(4)\n",
    "knn.fit(X_train, y_train)\n",
    "print('Train score for KNN: {:.4f}'.format(knn.score(X_train, y_train)))\n",
    "print('Test score for KNN: {:.4f}'.format(knn.score(X_test, y_test)))\n",
    "k=knn.score(X_test, y_test)\n",
    "k=round(k,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mglearn\n",
    "X_b = X_train_transformed[100:150,[3,6]]\n",
    "y_b = y_train[100:150]\n",
    "\n",
    "knn = KNeighborsClassifier(4)\n",
    "knn.fit(X_b, y_b) \n",
    "\n",
    "mglearn.plots.plot_2d_separator(knn, X_b, fill=True, eps=0.5, alpha=.4)\n",
    "mglearn.discrete_scatter(X_b[:, 0], X_b[:, 1], y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "c_range = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "train_score_l1 = []\n",
    "train_score_l2 = []\n",
    "test_score_l1 = []\n",
    "test_score_l2 = []\n",
    "\n",
    "for c in c_range:\n",
    "    log_l1 = LogisticRegression(penalty = 'l1', C = c)\n",
    "    log_l2 = LogisticRegression(penalty = 'l2', C = c)\n",
    "    log_l1.fit(X_train_transformed, y_train)\n",
    "    log_l2.fit(X_train_transformed, y_train)\n",
    "    train_score_l1.append(log_l1.score(X_train_transformed, y_train))\n",
    "    train_score_l2.append(log_l2.score(X_train_transformed, y_train))\n",
    "    test_score_l1.append(log_l1.score(X_test_transformed, y_test))\n",
    "    test_score_l2.append(log_l2.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(c_range, train_score_l1, label = 'Train score, penalty = l1')\n",
    "plt.plot(c_range, test_score_l1, label = 'Test score, penalty = l1')\n",
    "plt.plot(c_range, train_score_l2, label = 'Train score, penalty = l2')\n",
    "plt.plot(c_range, test_score_l2, label = 'Test score, penalty = l2')\n",
    "plt.legend()\n",
    "plt.xlabel('Regularization parameter: C')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At penalty L1 and C=1000, we get good accuracy for logestic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "log_reg = LogisticRegression(penalty = 'l1', C = 100)\n",
    "log_reg.fit(X_train, y_train)\n",
    "print('Train score for Logistic regression: {:.4f}'.format(log_reg.score(X_train, y_train)))\n",
    "print('Test score for Logistic regression: {:.4f}'.format(log_reg.score(X_test, y_test)))\n",
    "pred_log_reg = log_reg.predict(X_test)\n",
    "lr=log_reg.score(X_test, y_test)\n",
    "lr=round(lr,2)\n",
    "lrcm=confusion_matrix(y_test, pred_log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "c_range = [0.001,0.01,0.1,1,2,4,10,20,40]\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for c in c_range:\n",
    "    clf = SVC(gamma='auto',C=c)\n",
    "    clf.fit(X_train_transformed,y_train)\n",
    "    train_score.append(clf.score(X_train_transformed, y_train))\n",
    "    test_score.append(clf.score(X_test_transformed, y_test))\n",
    "\n",
    "    \n",
    "plt.plot(c_range, train_score, label = 'Train score')\n",
    "plt.plot(c_range, test_score, label = 'Test score')\n",
    "plt.legend()\n",
    "plt.xlabel('Penalty Parameter C')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At C=5, we get good accuracy for linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto',C=5)\n",
    "clf.fit(X_train,y_train)\n",
    "print('Train score for Linear SVM: {:.4f}'.format(clf.score(X_train, y_train)))\n",
    "print('Test score for Linear SVM: {:.4f}'.format(clf.score(X_test, y_test)))\n",
    "lsvm=clf.score(X_test, y_test)\n",
    "lsvm=round(lsvm,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain score for Linear SVM: 0.9828\n",
    "Test score for Linear SVM: 0.9948"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernalized SVM - Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for c in c_range:\n",
    "    clf = SVC(gamma='auto',C=c,kernel='linear')\n",
    "    clf.fit(X_train_transformed,y_train)\n",
    "    train_score.append(clf.score(X_train_transformed, y_train))\n",
    "    test_score.append(clf.score(X_test_transformed, y_test))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(c_range, train_score, label = 'Train score')\n",
    "plt.plot(c_range, test_score, label = 'Test score')\n",
    "plt.legend()\n",
    "plt.xlabel('Penalty Parameter C')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#At C=2, we get good accuracy for SVM (kernel = linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto',C=2,kernel='linear')\n",
    "clf.fit(X_train_transformed,y_train)\n",
    "print('Train score for SVM(kernel=linear): {:.4f}'.format(clf.score(X_train_transformed, y_train)))\n",
    "print('Test score for SVM(kernel=linear): {:.4f}'.format(clf.score(X_test_transformed, y_test)))\n",
    "klsvm=clf.score(X_test_transformed, y_test)\n",
    "klsvm=round(klsvm,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernalised SVM - rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for c in c_range:\n",
    "    clf = SVC(gamma='auto',C=c,kernel='rbf')\n",
    "    clf.fit(X_train_transformed,y_train)\n",
    "    train_score.append(clf.score(X_train_transformed, y_train))\n",
    "    test_score.append(clf.score(X_test_transformed, y_test))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(c_range, train_score, label = 'Train score')\n",
    "plt.plot(c_range, test_score, label = 'Test score')\n",
    "plt.legend()\n",
    "plt.xlabel('Penalty Parameter C')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At C=40, we get good accuracy for SVM (kernel = rbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto',C=40,kernel='rbf')\n",
    "clf.fit(X_train_transformed,y_train)\n",
    "print('Train score for SVM(kernel=rbf): {:.4f}'.format(clf.score(X_train_transformed, y_train)))\n",
    "print('Test score for SVM(kernel=rbf): {:.4f}'.format(clf.score(X_test_transformed, y_test)))\n",
    "rbfsvm=clf.score(X_test_transformed, y_test)\n",
    "rbfsvm=round(rbfsvm,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernalised SVM - poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for c in c_range:\n",
    "    clf = SVC(gamma='auto',C=c,kernel='poly',degree=2)\n",
    "    clf.fit(X_train_transformed,y_train)\n",
    "    train_score.append(clf.score(X_train_transformed, y_train))\n",
    "    test_score.append(clf.score(X_test_transformed, y_test))\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(c_range, train_score, label = 'Train score')\n",
    "plt.plot(c_range, test_score, label = 'Test score')\n",
    "plt.legend()\n",
    "plt.xlabel('Penalty Parameter C')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto',C=14,kernel='poly',degree=2)\n",
    "clf.fit(X_train_transformed,y_train)\n",
    "print('Train score for SVM(kernel=poly): {:.4f}'.format(clf.score(X_train_transformed, y_train)))\n",
    "print('Test score for SVM(kernel=poly): {:.4f}'.format(clf.score(X_test_transformed, y_test)))\n",
    "polysvm=clf.score(X_test_transformed, y_test)\n",
    "polysvm=round(polysvm,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At C=14, we get good accuracy for SVM (kernel = poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "depth_range = [1,2,3,4,5,6,7,8,9,10]\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "for d in depth_range:\n",
    "    dtree = DecisionTreeClassifier(max_depth=d,random_state=0)\n",
    "    dtree.fit(X_train_transformed, y_train)\n",
    "    train_score.append(dtree.score(X_train_transformed, y_train))\n",
    "    test_score.append(dtree.score(X_test_transformed, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(depth_range, train_score, label = 'Train score')\n",
    "plt.plot(depth_range, test_score, label = 'Test score')\n",
    "plt.legend()\n",
    "plt.xlabel('Maximum depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At maximum tree depth = 7, we get good accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(max_depth=7,random_state=0)\n",
    "\n",
    "dtree.fit(X_train_transformed, y_train)\n",
    "print(\"Train score for Decision Tree: {:.3f}\".format(dtree.score(X_train_transformed, y_train)))\n",
    "print(\"Test score for Decision Tree: {:.3f}\".format(dtree.score(X_test_transformed, y_test)))\n",
    "dt=dtree.score(X_test_transformed, y_test)\n",
    "dt=round(dt,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Different Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "x = ['KNN', 'Logistic','SVM','SVM-linear','SVM-rbf','SVM-poly', 'Dec-Tree']\n",
    "models = [k,lr,lsvm,klsvm,rbfsvm,polysvm,dt]\n",
    "x_pos = [i for i, _ in enumerate(x)]\n",
    "plt.bar(x_pos, models, color='green')\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy for different classification models\")\n",
    "\n",
    "plt.xticks(x_pos, x)\n",
    "plt.ylim(ymax = 1.0, ymin = 0.8)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print('Accuracy for KNN Classification: {:.4f}'.format(k))\n",
    "print('Accuracy for Logistic regression: {:.4f}'.format(lr))\n",
    "print('Accuracy for Linear SVM Classification: {:.4f}'.format(lsvm))\n",
    "print('Accuracy for SVM(kernel=linear) Classification: {:.4f}'.format(klsvm))\n",
    "print('Accuracy for SVM(kernel=rbf) Classification: {:.4f}'.format(rbfsvm))\n",
    "print('Accuracy for SVM(kernel=poly) Classification: {:.4f}'.format(polysvm))\n",
    "print('Accuracy for Decision Tree: {:.4f}'.format(dt))\n",
    "print(\"Confusion matrix for logistic regression:\\n{}\".format(lrcm))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Comparison #####\n",
    "\n",
    "\n",
    "\n",
    "knn score \n",
    "without pca\n",
    "Train score for KNN: 0.9794\n",
    "Test score for KNN: 0.9742\n",
    "with pca\n",
    "Train score for KNN: 0.9794\n",
    "Test score for KNN: 0.9742\n",
    "____________________________________\n",
    "logistic reg\n",
    "Train score for Logistic regression: 0.9952\n",
    "Test score for Logistic regression: 0.9936\n",
    "\n",
    "with pca\n",
    "Train score for Logistic regression: 0.9966\n",
    "Test score for Logistic regression: 0.9948\n",
    "________________________\n",
    "linear svm\n",
    "At C=10, we get good accuracy for linear SVM\n",
    "Train score for Linear SVM: 0.9677\n",
    "Test score for Linear SVM: 0.9808\n",
    "after pca\n",
    "At C=5, we get good accuracy for linear SVM\n",
    "Train score for Linear SVM: 0.9845\n",
    "Test score for Linear SVM: 0.9897\n",
    "_________________________________\n",
    "\n",
    "Kernalized SVM - Linear\n",
    "\n",
    "#At C=2, we get good accuracy for SVM (kernel = linear)\n",
    "Train score for SVM(kernel=linear): 0.9661\n",
    "Test score for SVM(kernel=linear): 0.9808\n",
    "\n",
    "with PCA\n",
    "\n",
    "Train score for SVM(kernel=linear): 0.9794\n",
    "Test score for SVM(kernel=linear): 0.9948\n",
    "__________________________________\n",
    "\n",
    "Kernalised SVM - rbf\n",
    "At C=40, we get good accuracy for SVM (kernel = rbf)\n",
    "Train score for SVM(kernel=rbf): 0.9726\n",
    "Test score for SVM(kernel=rbf): 0.9808\n",
    "\n",
    "with PCA\n",
    "Train score for SVM(kernel=rbf): 0.9863\n",
    "Test score for SVM(kernel=rbf): 0.9897\n",
    "\n",
    "__________________________________\n",
    "Kernalised SVM - poly\n",
    "At C=20, we get good accuracy for SVM (kernel = poly)\n",
    "Train score for SVM(kernel=poly): 0.9597\n",
    "Test score for SVM(kernel=poly): 0.9679\n",
    "\n",
    "with PCA\n",
    "At C=14, we get good accuracy for SVM (kernel = poly)\n",
    "\n",
    "\n",
    "Train score for SVM(kernel=poly): 0.9622\n",
    "Test score for SVM(kernel=poly): 0.9536\n",
    "__________________________________\n",
    "Decision Tree\n",
    "\n",
    "At maximum tree depth = 6, we get good accuracy\n",
    "Train score for Decision Tree: 0.988\n",
    "Test score for Decision Tree: 0.954\n",
    "\n",
    "with PCA\n",
    "At maximum tree depth = 7, we get good accuracy\n",
    "Train score for Decision Tree: 0.995\n",
    "Test score for Decision Tree: 0.985\n",
    "_________________________________________\n",
    "\n",
    "\n",
    "____________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
